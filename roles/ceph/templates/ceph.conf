# MANAGED BY ANSIBLE - DO NOT HAND EDIT!

[global]
fsid = {{ceph_fsid}} 
mon_initial_members = {{ceph_mon_initial_members}} 
mon_host = {{ceph_mons}} 

osd objectstore = bluestore

auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
#auth_cluster_required = none
#auth_service_required = none
#auth_client_required = none

osd_pool_default_size = {{ceph_osd_pool_default_size}}

# only enable the layering feature
rbd_default_features = 1

#----------------------------------------------------------------------------------
#network: It's recommended that you use two physically separated networks for your
#Ceph cluster, which are referred to as the public and cluster networks respectively.
#Earlier in this chapter, we covered the need for two different networks. Let's now
#understand how we can define them in a Ceph configuration.
#----------------------------------------------------------------------------------

public network = {{ceph_public_network}}/{{ceph_public_netmask}}
cluster network = {{ceph_private_network}}/{{ceph_private_netmask}}

#----------------------------------------------------------------------------------
#max open files: If this parameter is in place and the Ceph cluster starts, it sets
#the max open file descriptors at the OS level. This keeps OSD daemons from running
#out of the file descriptors. The default value of this parameter is 0; you can set it as
#up to a 64-bit integer:
#----------------------------------------------------------------------------------
max open files = 131072

#----------------------------------------------------------------------------------
#osd pool default min size: This is the replication level in a degraded state. It
#sets the minimum number of replicas for the objects in pool in order to acknowledge
#a write operation from clients. The default value is 0:
#----------------------------------------------------------------------------------
osd pool default min size = 1

#----------------------------------------------------------------------------------
#osd pool default pg and osd pool default pgp: Make sure that the cluster
#has a realistic number of placement groups. The recommended value of placement
#group per osd is 100. Use this formula to calculate the PG count: (Total number
#of OSD * 100)/number of replicas
#For 10 OSD and a replica size of 3, the PG count should be under (10*100)/3 = 333:
#the PG and PGP number should be kept the same,
#You should know that these parameters do not change the PG and PGP numbers for existing
#pools; they are applied when you create a new pool without specifying the PG and PGP values.
#The default value is 8
#----------------------------------------------------------------------------------
osd pool default pg num = 512
osd pool default pgp num = 512
mon_pg_warn_max_per_osd = 1000

#----------------------------------------------------------------------------------
#osd pool default crush rule: The default CRUSH ruleset to use when creating a pool:
#The default value is -1
#----------------------------------------------------------------------------------
osd pool default crush rule = 0
debug_lockdep = 0/0
debug_context = 0/0
debug_crush = 0/0
debug_buffer = 0/0
debug_timer = 0/0
debug_filer = 0/0
debug_objecter = 0/0
debug_rados = 0/0
debug_rbd = 0/0
debug_journaler = 0/0
debug_objectcatcher = 0/0
debug_client = 0/0
debug_osd = 0/0
debug_optracker = 0/0
debug_objclass = 0/0
debug_filestore = 0/0
debug_journal = 0/0
debug_ms = 0/0
debug_monc = 0/0
debug_tp = 0/0
debug_auth = 0/0
debug_finisher = 0/0
debug_heartbeatmap = 0/0
debug_perfcounter = 0/0
debug_asok = 0/0
debug_throttle = 0/0
debug_mon = 0/0
debug_mgr = 0/0
debug_paxos = 0/0
debug_rgw = 0/0
debug_rocksdb = 0/0
debug_bdev = 0/0


[mon]
mon_allow_pool_delete= true
#----------------------------------------------------------------------------------
#mon osd down out interval: This is the number of seconds Ceph waits before
#marking a Ceph OSD Daemon "down" and "out" if it doesn't respond. This option
#comes in handy when your OSD nodes crash and reboot by themselves or after some
#short glitch in the network. You don't want your cluster to start rebalancing as soon as
#the problem comes, rather wait for a few minutes and see if the problem gets fixed:
#The default value is 300
#----------------------------------------------------------------------------------
mon_osd_down_out_interval = 600

#----------------------------------------------------------------------------------
#mon allow pool delete: To avoid the accidental deletion of the Ceph pool, set
#this parameter as false. This can be useful if you have many administrators managing
#the Ceph cluster, and you do not want to take any risk with client data:
#The default value is true
#----------------------------------------------------------------------------------
#mon_allow_pool_delete = false

#----------------------------------------------------------------------------------
#mon osd min down reporters: The Ceph OSD daemon can report to MON about
#its peer OSDs if they are down; by default, this value is 2. With this option, you can
#change the minimum number of Ceph OSD Daemons required to report a down Ceph
#OSD to the Ceph monitor. In a large cluster, it's recommended that you have this
#value larger than the default; 3 should be a good number:
#----------------------------------------------------------------------------------
mon_osd_min_down_reporters = 2

[osd]
#OSD General Settings
#----------------------------------------------------------------------------------
#osd mkfs options xfs: At the time of OSD creation, Ceph will use these xfs
#options to create the OSD filesystem:
#----------------------------------------------------------------------------------
#osd_mkfs_options_xfs = "-f -i size=2048"

# here lists some default value
# "osd_op_num_threads_per_shard_hdd": "1",
# "osd_op_num_threads_per_shard_ssd": "2",
# "osd_op_num_shards": "0",
# "osd_op_num_shards_hdd": "5",
# "osd_op_num_shards_ssd": "8",
osd_op_num_threads_per_shard_hdd = 8
osd_op_num_shards_ssd = 8
osd_op_num_threads_per_shard_ssd = 6

#bluestore
#----------------------------------------------------------------------------------
#----------------------------------------------------------------------------------
#enable experimental unrecoverable data corrupting features = bluestore rocksdb
#bluestore fsck on mount = true
#bluestore block db size = 67108864
#bluestore block wal size = 134217728
#bluestore block size = 5368709120
#osd objectstore = bluestore
#bluestore = true
#bluestore_extent_map_shard_min_size = 50
#bluefs_buffered_io = true
#bluestore_min_alloc_size = 65536
#bluestore_extent_map_shard_max_size = 200
#bluestore_extent_map_shard_target_size = 100
#bluestore_csum_type = none
#bluestore_max_bytes = 1073741824
#bluestore_wal_max_bytes = 2147483648
#bluestore_max_ops = 8192
#bluestore_wal_max_ops = 8192


#----------------------------------------------------------------------------------
#osd mount options xfs: It supplies the xfs filesystem mount options to
#OSD. When Ceph is mounting an OSD, it will use the following options for OSD
#filesystem mounting:
#----------------------------------------------------------------------------------
#osd_mount_options_xfs = "rw,noatime,inode64,logbufs=8,logbsize=256k,delaylog,allocsize=4M"

#----------------------------------------------------------------------------------
#osd max write size: The maximum size in megabytes an OSD can write at a time: 256MB
#The default value is 90
#----------------------------------------------------------------------------------
osd_max_write_size = 256

#----------------------------------------------------------------------------------
#osd client message size cap: The largest client data message in bytes that is
#allowed in memory: 1GB
#The default value is 524288000=500MB
#----------------------------------------------------------------------------------
osd_client_message_size_cap = 1073741824

#----------------------------------------------------------------------------------
#osd map dedup: Remove duplicate entries in the OSD map:
#The default value is true
#----------------------------------------------------------------------------------
osd_map_dedup = true

#----------------------------------------------------------------------------------
#osd op threads: The number of threads to service the Ceph OSD Daemon operations.
#Set it to 0 to disable it. Increasing the number may increase the request-processing rate:
#The default value is 2
#----------------------------------------------------------------------------------
osd_op_threads = 16

#----------------------------------------------------------------------------------
#osd disk threads: The number of disk threads that are used to perform
#background disk-intensive OSD operations such as scrubbing and snap trimming:
#The default value is 2
#----------------------------------------------------------------------------------
osd_disk_threads = 1

#----------------------------------------------------------------------------------
#osd disk thread ioprio class: It is used in conjunction with osd_disk_
#thread_ioprio_priority. This tunable can change the I/O scheduling class of
#the disk thread, and it only works with the Linux kernel CFQ scheduler. The possible
#values are idle, be, or rt:
#      idle: The disk thread will have lower priority than any other thread in the
#            OSD. It is useful when you want to slow down the scrubbing on an OSD that
#            is busy handling client requests.
#      be:  The disk threads have the same priority as other threads in the OSD.
#      rt:  The disk thread will have more priority than all the other threads. This
#           is useful when scrubbing is much needed, and it can be prioritized at the
#           expense of client operations.
#default is NULL, empty
#----------------------------------------------------------------------------------
osd_disk_thread_ioprio_class = idle

#----------------------------------------------------------------------------------
#osd disk thread ioprio priority: It's used in conjunction with osd_disk_
#thread_ioprio_class. This tunable can change the I/O scheduling priority of the
#disk thread ranging from 0 (highest) to 7 (lowest). If all OSDs on a given host are in
#class idle and are competing for I/O and not doing much operations, this parameter
#can be used to lower the disk thread priority of one OSD to 7 so that another OSD
#with the priority 0 can potentially scrub faster. Like osd_disk_thread_ioprio_
#class, this also works with the Linux kernel CFQ scheduler, default is -1
#----------------------------------------------------------------------------------
osd_disk_thread_ioprio_priority = 0

#OSD Journal settings
#----------------------------------------------------------------------------------
#osd journal size: Ceph's default osd journal size value is 0; you should
#use the osd_journal_size parameter to set the journal size. The journal size
#should be at least twice the product of the expected drive speed and filestore max
#sync interval. If you are using SSD journals, it's usually good to create journals larger
#than 10 Gb and increase the filestore min/max sync interval:
#The default value is 5120
#----------------------------------------------------------------------------------
#osd_journal_size = 40960

#----------------------------------------------------------------------------------
#journal max write bytes: The maximum number of bytes the journal can write at once.
#The default value is 10485760 = 10MB, set to about 1GB
#----------------------------------------------------------------------------------
#journal_max_write_bytes = 1073714824

#----------------------------------------------------------------------------------
#journal max write entries: The maximum number of entries the journal can
#write at once. The default value is 100
#----------------------------------------------------------------------------------
#journal_max_write_entries = 10000

#----------------------------------------------------------------------------------
#journal queue max ops: The maximum number of operations allowed in the
#journal queue at a given time:
#sudo ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok config show | grep journal_queue
#not found this parameter: journal_queue_max_ops
#----------------------------------------------------------------------------------
journal_queue_max_ops = 50000

#----------------------------------------------------------------------------------
#journal queue max bytes: The maximum number of bytes allowed in the journal
#queue at a given time:
#sudo ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok config show | grep journal_queue
#not found this parameter: journal_queue_max_bytes
#----------------------------------------------------------------------------------
journal_queue_max_bytes = 10485760000

#----------------------------------------------------------------------------------
#journal dio: This enables direct i/o to the journal. It requires journal block
#align to be set to true:
#The default value is true
#----------------------------------------------------------------------------------
journal_dio = true

#----------------------------------------------------------------------------------
#journal aio: This enables using libaio for asynchronous writes to the journal. It
#requires journal dio to be set to true:
#The default value is true
#----------------------------------------------------------------------------------
journal_aio = true

#----------------------------------------------------------------------------------
#journal block align: This block aligns write operations. It's required for dio
#and aio.
#The default value is true
#----------------------------------------------------------------------------------
journal_block_align = true

#OSD Filestore settings
#----------------------------------------------------------------------------------
#filestore merge threshold: This enables using libaio for asynchronous writes
#to the journal. It requires journal dio to be set to true:
#The default value is 10
#----------------------------------------------------------------------------------
filestore_merge_threshold = 40

#----------------------------------------------------------------------------------
#filestore split multiple: The maximum number of files in a subdirectory
#before splitting into child directories:
#The default value is 2
#----------------------------------------------------------------------------------
filestore_split_multiple = 8

#----------------------------------------------------------------------------------
#filestore op threads: The number of filesystem operation threads that execute
#in parallel:
#The default value is 2
#----------------------------------------------------------------------------------
filestore_op_threads = 32

#----------------------------------------------------------------------------------
#filestore xattr use omap: Uses the object map for XATTRS (extended
#attributes). Needs to be set to true for the ext4 file systems:
#The default value is false
#----------------------------------------------------------------------------------
#filestore_xattr_use_omap = true

#----------------------------------------------------------------------------------
#filestore sync interval: In order to create a consistent commit point, the
#filestore needs to quiesce write operations and do a syncfs() operation, which
#syncs data from the journal to the data partition and thus frees the journal. A more
#frequently performed sync operation reduces the amount of data that is stored
#in a journal. In such cases, the journal becomes underutilized. Configuring less
#frequent syncs allows the filesystem to coalesce small writes better, and we might get
#improved performance. The following parameters define the minimum and maximum
#time period between two syncs:
#The default value filestore_min_sync_interval is 0.01
#The default value filestore_max_sync_interval is 5
#----------------------------------------------------------------------------------
filestore_min_sync_interval = 10
filestore_max_sync_interval = 15

#----------------------------------------------------------------------------------
#filestore queue max ops: The maximum number of operations that a filestore
#can accept before blocking new operations from joining the queue:
#The default value is 50
#----------------------------------------------------------------------------------
filestore_queue_max_ops = 2500

#----------------------------------------------------------------------------------
#filestore queue max bytes: The maximum number of bytes in an operation:
#The default value is 104857600
#----------------------------------------------------------------------------------
filestore_queue_max_bytes = 10485760

#----------------------------------------------------------------------------------
#filestore queue committing max ops: The maximum number of operations
#the filestore can commit:
#sudo ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok config show | grep journal_queue
#not found this parameter: filestore_queue_committing_max_ops
#----------------------------------------------------------------------------------
filestore_queue_committing_max_ops = 5000

#----------------------------------------------------------------------------------
#filestore queue committing max bytes: The maximum number of bytes the
#filestore can commit:
#sudo ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok config show | grep journal_queue
#not found this parameter: filestore_queue_committing_max_bytes
#----------------------------------------------------------------------------------
filestore_queue_committing_max_bytes = 10485760000

#OSD Recovery settings
#----------------------------------------------------------------------------------
#These settings should be used when you want performance over recovery or vice versa. If your
#Ceph cluster is unhealthy and is under recovery, you might not get its usual performance, as
#OSDs will be busy with recovery. If you still prefer performance over recovery, you can reduce
#the recovery priority to keep OSDs less occupied with recovery. You can also set these values
#if you want a quick recovery for your cluster, helping OSDs to perform recovery faster.
#----------------------------------------------------------------------------------
#osd recovery max active: The number of active recovery requests per OSD at a given moment:
#The default value is 3
#----------------------------------------------------------------------------------
osd_recovery_max_active = 1

#----------------------------------------------------------------------------------
#osd recovery max single start: This is used in conjunction with osd_
#recovery_max_active. To understand this, let's assume osd_recovery_max_
#single_start is equal to 1, and osd_recovery_max_active is equal to 3. In
#this case, it means that the OSD will start a maximum of one recovery operation at a
#time, out of a total of three operations active at that time:
#The default value is 1
#----------------------------------------------------------------------------------
osd_recovery_max_single_start = 1

#----------------------------------------------------------------------------------
#osd recovery op priority: This is the priority set for the recovery operation.
#The lower the number, the higher the recovery priority:
#The default value is 3
#----------------------------------------------------------------------------------
osd_recovery_op_priority = 10

#----------------------------------------------------------------------------------
#osd recovery max chunk: The maximum size of a recovered chunk of data
#in bytes: The default value is 8388608
#----------------------------------------------------------------------------------
osd_recovery_max_chunk = 1048576

#----------------------------------------------------------------------------------
#osd recovery threads: The number of threads needed for recovering data:
#The default value is 1
#----------------------------------------------------------------------------------
osd_recovery_threads = 1

#OSD Backfilling settings
#----------------------------------------------------------------------------------
#OSD backfilling settings allow Ceph to set backfilling operations at a lower priority than
#requests to read and write:
#osd max backfills: The maximum number of backfills allowed to or from a single OSD:
#The default value is 1
#----------------------------------------------------------------------------------
osd_max_backfills = 2

#----------------------------------------------------------------------------------
#osd backfill scan min: The minimum number of objects per backfill scan:
#The default value is 64
#----------------------------------------------------------------------------------
osd_backfill_scan_min = 8

#----------------------------------------------------------------------------------
#osd backfill scan max: The maximum number of objects per backfill scan:
#The default value is 512
#----------------------------------------------------------------------------------
osd_backfill_scan_max = 64

#OSD scrubbing settings
#----------------------------------------------------------------------------------
#OSD Scrubbing is important for maintaining data integrity, but it can reduce performance.
#You can adjust the following settings to increase or decrease scrubbing operations:
#osd max scrubs: The maximum number of simultaneous scrub operations for a
#Ceph OSD daemon:
#The default value is 1
#----------------------------------------------------------------------------------
osd_max_scrubs = 1

#----------------------------------------------------------------------------------
#osd scrub sleep: The time in seconds that scrubbing sleeps between two
#consecutive scrubs:
#The default value is 0
#----------------------------------------------------------------------------------
osd_scrub_sleep = 1

#----------------------------------------------------------------------------------
#osd scrub chunk min: The minimum number of data chunks an OSD should
#perform scrubbing on:
#The default value is  5
#----------------------------------------------------------------------------------
osd_scrub_chunk_min = 1

#----------------------------------------------------------------------------------
#osd scrub chunk max: The maximum number of data chunks an OSD should
#perform scrubbing on:
#The default value is 25
#----------------------------------------------------------------------------------
osd_scrub_chunk_max = 5

#----------------------------------------------------------------------------------
#osd deep scrub stride: The read size in bytes while doing a deep scrub:
#The default value is 524288
#----------------------------------------------------------------------------------
osd_deep_scrub_stride = 1048576

#----------------------------------------------------------------------------------
#osd scrub begin hour: The earliest hour that scrubbing can begin. This is used
#in conjunction with osd_scrub_end_hour to define a scrubbing time window:
#The default value is 0
#----------------------------------------------------------------------------------
osd_scrub_begin_hour = 19

#----------------------------------------------------------------------------------
#osd scrub end hour: This is the upper bound when the scrubbing can be
#performed. This works in conjunction with osd_scrub_begin_hour to define a
#scrubbing time window:
#The default value is 24
#----------------------------------------------------------------------------------
osd_scrub_end_hour = 7

#OSD async messenger settings
#----------------------------------------------------------------------------------
#"ms_type": "simple", Messenger is the Ceph network layer implementation. Currently
#Ceph supports three messenger type “simple”, “async” and “xio”. The latter two are
#both experiment features and shouldn’t use them in production environment.
#from http://docs.ceph.com/docs/master/dev/messenger/
#"ms_async_op_threads": "3", number of worker processing threads for async messenger created on init
#"ms_async_max_op_threads": "5", max number of worker processing threads for async messenger
#"ms_dispatch_throttle_bytes": "104857600", Throttles total size of messages waiting to be dispatched.
#----------------------------------------------------------------------------------
#ms_type = async
ms_async_op_threads = 8
ms_async_max_op_threads = 8
ms_dispatch_throttle_bytes = 104857600000
#ms_type = async+rdma
#ms_async_rdma_device_name=mlx4_0
#ms_async_rdma_roce_ver=0

bluestore = true
[osd.0]
host = ceph-rep-05
osd data = /var/lib/ceph/osd/ceph-0/
bluestore_block_path = spdk:5780A003A5KD
bluestore_block_db_path = ""
bluestore_block_db_size = 0
bluestore_block_db_create = false
bluestore_block_wal_path = ""
bluestore_block_wal_size = 0
bluestore_block_wal_create = false
bluestore_spdk_mem = 2048
#bluestore_spdk_mem = 2048

[Client]
#Client tuning
#----------------------------------------------------------------------------------
#The client tuning parameters should be defined under the [client] section of your Ceph
#configuration file. Usually, this [client] section should also be present in the Ceph
#configuration file hosted on the client node:
#rbd cache: Enable caching for the RADOS Block Device (RBD):
#The default value is true
#----------------------------------------------------------------------------------
rbd_cache = true

#----------------------------------------------------------------------------------
#rbd cache writethrough until flush: Start out in the write-through
#mode, and switch to writeback after the first flush request is received:
#The default value is true
#----------------------------------------------------------------------------------
rbd_cache_writethrough_until_flush = true

#----------------------------------------------------------------------------------
#rbd concurrent management ops: The number of concurrent management
#operations that can be performed on rbd:
#The default value is 10
#----------------------------------------------------------------------------------
rbd_concurrent_management_ops = 10

#----------------------------------------------------------------------------------
#rbd cache size: The rbd cache size in bytes:
#The default value is 33554432 = 32M
#----------------------------------------------------------------------------------
rbd_cache_size = 67108864 #64M

#----------------------------------------------------------------------------------
#rbd cache max dirty: The limit in bytes at which the cache should trigger a
#writeback. It should be less than rbd_cache_size:
#The default value is 25165824 = 24M
#----------------------------------------------------------------------------------
rbd_cache_max_dirty = 50331648 #48M

#----------------------------------------------------------------------------------
#rbd cache target dirty: The dirty target before the cache begins writing data
#to the backing store:
#The default value is 16777216 = 16M
#----------------------------------------------------------------------------------
rbd_cache_target_dirty = 33554432 #32M

#----------------------------------------------------------------------------------
#rbd cache max dirty age: The number of seconds that the dirty data is in the
#cache before writeback starts:
#The default value is 1
#----------------------------------------------------------------------------------
rbd_cache_max_dirty_age = 2

#----------------------------------------------------------------------------------
#rbd default format: This uses the second rbd format, which is supported by
#librbd and the Linux kernel since version 3.11. This adds support for cloning and is
#more easily extensible, allowing more features in the future:
#The default value is 2
#----------------------------------------------------------------------------------
rbd_default_format = 2

#----------------------------------------------------------------------------------

[client.images]
keyring = /etc/ceph/ceph.client.images.keyring

[client.nova]
keyring = /etc/ceph/ceph.client.nova.keyring

[client.cinder]
keyring = /etc/ceph/ceph.client.cinder.keyring

